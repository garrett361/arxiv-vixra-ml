{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"large_abstract_recurrent_one_hot_next_language_model.ipynb","provenance":[{"file_id":"1XCIuPBVtvIllx6TLL78gOhA6Kq-2LrOL","timestamp":1636404967872}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMFEzEA6HvyDHgd2uqa2OFt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"19f0584223e748559c91fc00e2f68d17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3f6d1049633a4b358767fdfa9d54d7a6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b607ae8a81fc4bd8a47da35e2992faba","IPY_MODEL_b5d87d527bc7442a928da11886727be3","IPY_MODEL_cfa9cee378804406a35e314478a2d50c"]}},"3f6d1049633a4b358767fdfa9d54d7a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"b607ae8a81fc4bd8a47da35e2992faba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b7ee686b26874244bcf29d7e472bb452","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Validation sanity check: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85ee5b3076024f0e9dd582e7e5aa064e"}},"b5d87d527bc7442a928da11886727be3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2d5ef34935754853b48c659579f9f8f7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eec8cc9e2c884512a3b99360105207cf"}},"cfa9cee378804406a35e314478a2d50c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c7549eba91f4852ad9806492dff7300","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:04&lt;00:00,  2.23s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b935142ac40e4f1c901ce16e7dae7b52"}},"b7ee686b26874244bcf29d7e472bb452":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85ee5b3076024f0e9dd582e7e5aa064e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d5ef34935754853b48c659579f9f8f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eec8cc9e2c884512a3b99360105207cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c7549eba91f4852ad9806492dff7300":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b935142ac40e4f1c901ce16e7dae7b52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a4bb51d5f9f84925a8edf2fc86e76135":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_de45a88569a04711a88463ba953e1dac","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_caa70790957f49f697b6fcfc5b239efe","IPY_MODEL_3017032257cf4bbfad974de0386656ef","IPY_MODEL_ed8f992a745743cc8849d54fb71dce8f"]}},"de45a88569a04711a88463ba953e1dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"caa70790957f49f697b6fcfc5b239efe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d5fefc6ba60d4ec084f487fcdddb7fe5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch 0:  76%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_525a3d162baa400987e61175272da739"}},"3017032257cf4bbfad974de0386656ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a226045f68644d06851eebe8f4a53e73","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1430,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1080,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_955e6ccf30e74410bd769de447302273"}},"ed8f992a745743cc8849d54fb71dce8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a1864bb7204f4d4091fa76f4211fffad","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1080/1430 [28:52&lt;09:21,  1.60s/it, loss=1.57, split_idx=7, v_num=48v3, train_batch_loss=1.570]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc1ca3c4a70d4bc3a9040c0911f8d0bf"}},"d5fefc6ba60d4ec084f487fcdddb7fe5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"525a3d162baa400987e61175272da739":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a226045f68644d06851eebe8f4a53e73":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"955e6ccf30e74410bd769de447302273":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1864bb7204f4d4091fa76f4211fffad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cc1ca3c4a70d4bc3a9040c0911f8d0bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"YtasH2tBWrTK"},"source":["# Simple Recurrent Language Model\n","\n","Predicting the next token."]},{"cell_type":"markdown","metadata":{"id":"z225dbwoaygs"},"source":["# Imports and Setup\n","\n","Common imports and standardized code for importing the relevant data, models, etc., in order to minimize copy-paste/typo errors."]},{"cell_type":"markdown","metadata":{"id":"cBD3U5YCt7-s"},"source":["\n","Set the relevant text field (`'abstract'` or `'title'`) and whether we are working with `'one-hot'` or `'tokenized'` text.  "]},{"cell_type":"code","metadata":{"id":"7N6wzc1S1f91","executionInfo":{"status":"ok","timestamp":1642222075697,"user_tz":300,"elapsed":378,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["TEXT_FIELD = 'abstract'\n","TEXT_ENCODING = 'one-hot'\n","assert TEXT_FIELD in ('abstract', 'title'), 'TEXT_FIELD must be one of \"title\" or \"abstract\".'\n","assert TEXT_ENCODING in ('one-hot', 'tokenized'), 'TEXT_ENCODING must be one of \"one-hot\" or \"tokenized\".'\n","# The above choices determine the relevant sequence length of the data.\n","SEQ_LEN = 512 if TEXT_ENCODING == 'tokenized' else 1024"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfUISDFhbmZB"},"source":["Imports and colab setup"]},{"cell_type":"code","metadata":{"id":"beaGM0OZDGBe","executionInfo":{"status":"ok","timestamp":1642222091386,"user_tz":300,"elapsed":15403,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["%%capture import_capture --no-stder\n","# Jupyter magic methods\n","# For auto-reloading when external modules are changed\n","%load_ext autoreload\n","%autoreload 2\n","# For showing plots inline\n","%matplotlib inline\n","\n","# pip installs needed in Colab for arxiv_vixra_models\n","!pip install wandb\n","!pip install pytorch-lightning\n","!pip install unidecode\n","# Update sklearn\n","!pip uninstall scikit-learn -y\n","!pip install -U scikit-learn\n","\n","from copy import deepcopy\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","pd.set_option(u'float_format', '{:f}'.format)\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","import seaborn as sns\n","import torch\n","import wandb"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmuHdurUyvjK"},"source":["`wandb` log in:"]},{"cell_type":"code","metadata":{"id":"3DfWm5EfyyV-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222091387,"user_tz":300,"elapsed":37,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"ab6b5127-7496-493b-a5d7-41a8c8e71c74"},"source":["wandb.login()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"kamviAnIb068"},"source":["Google drive access"]},{"cell_type":"code","metadata":{"id":"Q2rHoHQkVKqH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222094337,"user_tz":300,"elapsed":2957,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"a766d433-a3a5-49f3-d24e-002ffddba34f"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","# Enter the relevant foldername\n","FOLDERNAME = '/content/drive/My Drive/ML/arxiv_vixra'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","# For importing modules stored in FOLDERNAME or a subdirectory thereof:\n","import sys\n","sys.path.append(FOLDERNAME)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"K9ga5ia9cFRw"},"source":["Import my models, loaders, and utility functions:"]},{"cell_type":"code","metadata":{"id":"j8U3Ki7mbcw7","executionInfo":{"status":"ok","timestamp":1642222094339,"user_tz":300,"elapsed":11,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["import arxiv_vixra_models as avm"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CXP2Q2FoaryB"},"source":["Set the model, datamodule, and text utils to be instantianted in the notebook"]},{"cell_type":"code","metadata":{"id":"UZfn8eELa0sg","executionInfo":{"status":"ok","timestamp":1642222094340,"user_tz":300,"elapsed":10,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["notebook_model = avm.LitOneHotCharRNNNextLM\n","notebook_datamodule = avm.OneHotCharDataModuleNextLM\n","notebook_encoder = avm.str_to_one_hot \n","notebook_decoder = avm.one_hot_to_str \n","notebook_wandb_callback = avm.WandbTextGenerationCallback"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAjCZ39tcQ1Y"},"source":["Copy data to cwd for speed."]},{"cell_type":"code","metadata":{"id":"V4b1KZSYV8rP","executionInfo":{"status":"ok","timestamp":1642222107496,"user_tz":300,"elapsed":13165,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["train_data_file_name = 'large_filtered_normalized_data_train.feather'\n","val_data_file_name = 'balanced_filtered_normalized_data_validation.feather'\n","SUBDIR = '/data/data_splits/'\n","train_data_path = FOLDERNAME + SUBDIR + train_data_file_name\n","val_data_path = FOLDERNAME + SUBDIR + val_data_file_name\n","if TEXT_ENCODING == 'one-hot':\n","    tokens_file_name = 'normalized_char_set.feather'\n","else:\n","    tokens_file_name = 'balanced_title_normalized_vocab.feather'\n","tokens_path = FOLDERNAME + SUBDIR + tokens_file_name\n","!cp '{train_data_path}' .\n","!cp '{val_data_path}' .\n","!cp '{tokens_path}' .\n","train_data_df = pd.read_feather(train_data_file_name)\n","val_data_df = pd.read_feather(val_data_file_name)\n","tokens_df = pd.read_feather(tokens_file_name)\n","if TEXT_ENCODING == 'one-hot':\n","    text_to_idx = dict(zip(tokens_df.char.values, np.arange(len(tokens_df))))\n","else:\n","    # 0 and 1 are reserved for padding and <UNK> for embeddings and not included\n","    # in tokens_df\n","    text_to_idx = dict(zip(tokens_df.word.values, np.arange(2, len(tokens_df) + 2)))\n","    text_to_idx['<PAD>'] = 0\n","    text_to_idx['<UNK>'] = 1\n","idx_to_text = {val: key for key, val in text_to_idx.items()}\n","if TEXT_FIELD == 'title':\n","    train_text_file_name = 'concatenated_large_normalized_train_title.txt'\n","    val_text_file_name = 'concatenated_balanced_normalized_validation_title.txt'\n","else:\n","    train_text_file_name = 'concatenated_large_normalized_train_abstract.txt'\n","    val_text_file_name = 'concatenated_balanced_normalized_validation_abstract.txt'\n","with open(FOLDERNAME + SUBDIR + train_text_file_name, 'r') as f:\n","    train_text = f.read().strip()\n","with open(FOLDERNAME + SUBDIR + val_text_file_name, 'r') as f:\n","    val_text = f.read().strip()"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRfQtriZWa1P"},"source":["Computing specs. Save the number of processors to pass as `num_workers` into the Datamodule and cuda availability for other flags."]},{"cell_type":"code","metadata":{"id":"yuUpH52TUAG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222107497,"user_tz":300,"elapsed":41,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"f3bcca1e-3d0b-45cd-ce04-5dea8186e5a8"},"source":["# GPU. Save availability to IS_CUDA_AVAILABLE.\n","gpu_info= !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","  IS_CUDA_AVAILABLE = False\n","else:\n","  print(f\"GPU\\n{50 * '-'}\\n\", gpu_info, '\\n')\n","  IS_CUDA_AVAILABLE = True\n","\n","# Memory.\n","from psutil import virtual_memory, cpu_count\n","ram_gb = virtual_memory().total / 1e9\n","print(f\"Memory\\n{50 * '-'}\\n\", 'Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb), '\\n')\n","\n","# CPU.\n","print(f\"CPU\\n{50 * '-'}\\n\", f'CPU Processors: {cpu_count()}')\n","# Determine the number of workers to use in the datamodule\n","NUM_PROCESSORS = cpu_count()"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU\n","--------------------------------------------------\n"," Sat Jan 15 04:48:26 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    39W / 300W |  10001MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+ \n","\n","Memory\n","--------------------------------------------------\n"," Your runtime has 54.8 gigabytes of available RAM\n"," \n","\n","CPU\n","--------------------------------------------------\n"," CPU Processors: 8\n"]}]},{"cell_type":"markdown","metadata":{"id":"yP-xjLY4cEEL"},"source":["Use notebook name as `wandb` `project` string. Remove the file extension and any \"Copy of\" or \"Kopie van\" text which arises from copying notebooks and running in parallel. The `entity` needed for various `wandb` calls is just the `wandb` user name."]},{"cell_type":"code","metadata":{"id":"yy8UZlYodrhO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222107498,"user_tz":300,"elapsed":14,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"eca4f480-d98b-40b6-9782-c5e519f3d9c3"},"source":["from requests import get\n","PROJECT = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n","PROJECT = PROJECT.replace('.ipynb', '').replace('Kopie%20van%20', '').replace('Copy%20of%20', '')\n","print(PROJECT)\n","ENTITY = 'garrett361'"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["large_abstract_recurrent_one_hot_next_language_model\n"]}]},{"cell_type":"markdown","metadata":{"id":"2HVB4e-xchSi"},"source":["# Model Testing\n","\n","Setting hyperparameters and performing a small test run."]},{"cell_type":"markdown","metadata":{"id":"E0e7seomcjud"},"source":["Dictionary args for model and datamodule."]},{"cell_type":"code","metadata":{"id":"GLg5mLHchpqp","executionInfo":{"status":"ok","timestamp":1642222107498,"user_tz":300,"elapsed":9,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["model_args_dict = {'seq_len': SEQ_LEN,\n","                  'tokens': tokens_df,\n","                  'num_layers': 2,\n","                  'hidden_size': 512,\n","                  'rnn_type': 'GRU',\n","                  'fc_dims': None,\n","                  'zero_fc_bias_init': True,\n","                  'truncated_bptt_steps': 128\n","                  }\n","\n","data_args_dict = {'seq_len': SEQ_LEN,\n","                 'train_text': train_text,\n","                 'val_text': val_text,\n","                 'tokens': tokens_df, \n","                 'num_workers': NUM_PROCESSORS,\n","                 'batch_size': 128,\n","                 'pin_memory': IS_CUDA_AVAILABLE,\n","                 'persistent_workers': True,\n","                 }"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okEcWXWWxYb0"},"source":["Small test run."]},{"cell_type":"code","metadata":{"id":"qhOY0cnMvnX-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222120458,"user_tz":300,"elapsed":12969,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"16c4424b-0d40-46e5-bea5-abe9ecbb64cd"},"source":["small_data_module = notebook_datamodule(**data_args_dict)\n","small_data_module.setup()\n","small_loader = small_data_module.train_dataloader()\n","small_inputs, small_targets = next(iter(small_loader))\n","# Print the first few input texts\n","for input, target in  zip(small_inputs[:3], small_targets[:3]):\n","    sample_text = notebook_decoder(input, idx_to_text)\n","    sample_target = ''.join(idx_to_text[ch.item()] for ch in target)\n","    print(f\"input  text: {sample_text}\",\n","          f\"target text: {sample_target}\",\n","          f'input, target lens: {len(sample_text), len(sample_target)}',\n","          sep='\\n')\n","small_model = notebook_model(**model_args_dict)\n","print('Model layers:', small_model)\n","small_preds, small_losses, _ = small_model.scores_loss_hiddens(small_inputs, small_targets)\n","print('\\npreds shape:', small_preds.shape)\n","print('\\nactual loss:', small_losses.item())\n","print('\\nexpected approx loss', np.log(len(tokens_df)))"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["input  text: three - dimensional micromagnetic model including demagnetisation effects , we demonstrate that skyrmionic textures are the lowest energy states in helimagnetic thin film nanostructures at zero external magnetic field and in absence of magnetocrystalline anisotropy . we also report the regions of metastability for non - ground state equilibrium configurations . we show that bistable skyrmionic textures undergo hysteretic behaviour between two energetically equivalent skyrmionic states with different core orientation , even in absence of both magnetocrystalline and demagnetisation - based shape anisotropies , suggesting the existence of dzyaloshinskii - moriya - based shape anisotropy . finally , we show that the skyrmionic texture core reversal dynamics is facilitated by the bloch point occurrence and propagation . the aim of this article is to study degeneration of the variations of hodge structure associated to a proper k \\ \" ahler semistable morphism . we prove that the weight filtrations constructed in th\n","target text: hree - dimensional micromagnetic model including demagnetisation effects , we demonstrate that skyrmionic textures are the lowest energy states in helimagnetic thin film nanostructures at zero external magnetic field and in absence of magnetocrystalline anisotropy . we also report the regions of metastability for non - ground state equilibrium configurations . we show that bistable skyrmionic textures undergo hysteretic behaviour between two energetically equivalent skyrmionic states with different core orientation , even in absence of both magnetocrystalline and demagnetisation - based shape anisotropies , suggesting the existence of dzyaloshinskii - moriya - based shape anisotropy . finally , we show that the skyrmionic texture core reversal dynamics is facilitated by the bloch point occurrence and propagation . the aim of this article is to study degeneration of the variations of hodge structure associated to a proper k \\ \" ahler semistable morphism . we prove that the weight filtrations constructed in the\n","input, target lens: (1024, 1024)\n","input  text: onomy , thousands of new multiply imaged quasars are expected to be discovered and monitored regularly . light curves from the images of gravitationally lensed quasars are further affected by superimposed variability due to microlensing . in order to disentangle the microlensing from the intrinsic variability of the light curves , the time delays between the multiple images have to be accurately measured . the resulting microlensing light curves can then be analyzed to reveal information about the background source , such as the size of the quasar accretion disc . in this paper we present the most extensive and coherent collection of simulated microlensing light curves ; we have generated $ > 2 . 5 $ billion light curves using the gerlumph high resolution microlensing magnification maps . our simulations can be used to : train algorithms to measure lensed quasar time delays , plan future monitoring campaigns , and study light curve properties throughout parameter space . our data are openly available to the c\n","target text: nomy , thousands of new multiply imaged quasars are expected to be discovered and monitored regularly . light curves from the images of gravitationally lensed quasars are further affected by superimposed variability due to microlensing . in order to disentangle the microlensing from the intrinsic variability of the light curves , the time delays between the multiple images have to be accurately measured . the resulting microlensing light curves can then be analyzed to reveal information about the background source , such as the size of the quasar accretion disc . in this paper we present the most extensive and coherent collection of simulated microlensing light curves ; we have generated $ > 2 . 5 $ billion light curves using the gerlumph high resolution microlensing magnification maps . our simulations can be used to : train algorithms to measure lensed quasar time delays , plan future monitoring campaigns , and study light curve properties throughout parameter space . our data are openly available to the co\n","input, target lens: (1024, 1024)\n","input  text:  call the interpretation problem , whereby any rule we give the machine is open to infinite interpretation in ways that we might morally disapprove of , and that the interpretation problem in artificial intelligence is an illustration of wittgenstein ' s general claim that no rule can contain the criteria for its own application . using games as an example , we attempt to define the structure of normative spaces and argue that any rule - following within a normative space is guided by values that are external to that space and which cannot themselves be represented as rules . in light of this problem , we analyse the types of mistakes an artificial moral agent could make and we make suggestions about how to build morality into machines by getting them to interpret the rules we give in accordance with these external values , through explicit moral reasoning and the presence of structured values , the adjustment of causal power assigned to the agent and interaction with human agents , such that the machine deve\n","target text: call the interpretation problem , whereby any rule we give the machine is open to infinite interpretation in ways that we might morally disapprove of , and that the interpretation problem in artificial intelligence is an illustration of wittgenstein ' s general claim that no rule can contain the criteria for its own application . using games as an example , we attempt to define the structure of normative spaces and argue that any rule - following within a normative space is guided by values that are external to that space and which cannot themselves be represented as rules . in light of this problem , we analyse the types of mistakes an artificial moral agent could make and we make suggestions about how to build morality into machines by getting them to interpret the rules we give in accordance with these external values , through explicit moral reasoning and the presence of structured values , the adjustment of causal power assigned to the agent and interaction with human agents , such that the machine devel\n","input, target lens: (1024, 1024)\n","Model layers: LitOneHotCharRNNNextLM(\n","  (train_metrics_dict): ModuleDict(\n","    (train_acc): Accuracy()\n","    (train_precision): Precision()\n","    (train_recall): Recall()\n","    (train_specificity): Specificity()\n","  )\n","  (val_metrics_dict): ModuleDict(\n","    (val_acc): Accuracy()\n","    (val_precision): Precision()\n","    (val_recall): Recall()\n","    (val_specificity): Specificity()\n","  )\n","  (test_metrics_dict): ModuleDict(\n","    (test_acc): Accuracy()\n","    (test_precision): Precision()\n","    (test_recall): Recall()\n","    (test_specificity): Specificity()\n","  )\n","  (rnn): GRU(69, 512, num_layers=2, batch_first=True)\n","  (fc_layers): ModuleList(\n","    (0): Linear(in_features=512, out_features=69, bias=True)\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","preds shape: torch.Size([128, 69, 1024])\n","\n","actual loss: 4.23287296295166\n","\n","expected approx loss 4.23410650459726\n"]}]},{"cell_type":"code","metadata":{"id":"lLdlEdmO83mU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222120459,"user_tz":300,"elapsed":29,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"6a90570b-4dfc-44ef-c259-51c392da0714"},"source":["# pl implements gradient clipping through the Trainer.\n","small_trainer = Trainer(gpus=-1 if IS_CUDA_AVAILABLE else 0,\n","                        max_epochs=1,\n","                        gradient_clip_val=1\n","                        )"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n"]}]},{"cell_type":"markdown","metadata":{"id":"97-nbRSP87fY"},"source":["A `LR finder stopped early due to diverging loss.` here may be due to having too large a batch size, i.e., not enough samples from the datamodule; [see this github discussion](https://github.com/PyTorchLightning/pytorch-lightning/issues/5044)"]},{"cell_type":"code","metadata":{"id":"wE0laRH4samb","executionInfo":{"status":"ok","timestamp":1642222121269,"user_tz":300,"elapsed":816,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}}},"source":["# small_trainer_lr_finder = small_trainer.tuner.lr_find(small_model, datamodule=small_data_module, min_lr=1e-6, max_lr=1e-1)\n","# small_trainer_lr_finder_plot = small_trainer_lr_finder.plot(suggest=True)\n","# small_trainer_suggested_lr = small_trainer_lr_finder.suggestion()\n","# print(f'Suggested lr: {small_trainer_suggested_lr}')"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"b7DPjsjTnlso"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"OjBi9bSrYa4b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642222121275,"user_tz":300,"elapsed":33,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"b99e8caa-39b2-4793-d735-f8e484ee65e5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n"]}],"source":["cyclic_lr_scheduler_args = {'base_lr': 5e-3,\n","                            'max_lr': 7e-2,\n","                            'step_size_up': 400,\n","                            'cycle_momentum': False}\n","plateau_lr_scheduler_args = {'verbose': True,\n","                             'patience': 32,\n","                             'factor': .5,\n","                             'mode': 'min'}\n","\n","model_args_dict['save_models_to_wandb'] =True\n","model_args_dict['lr'] = 1e-2\n","model_args_dict['lr_scheduler'] = 'plateau'\n","model_args_dict['lr_scheduler_args'] = plateau_lr_scheduler_args\n","model_args_dict['lr_scheduler_monitor'] = 'train_batch_loss'\n","model = notebook_model(**model_args_dict)\n","\n","data_args_dict['batch_size'] = 1024\n","datamodule = notebook_datamodule(**data_args_dict)"]},{"cell_type":"markdown","metadata":{"id":"R2nS87PG92Aw"},"source":["Training:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["19f0584223e748559c91fc00e2f68d17","3f6d1049633a4b358767fdfa9d54d7a6","b607ae8a81fc4bd8a47da35e2992faba","b5d87d527bc7442a928da11886727be3","cfa9cee378804406a35e314478a2d50c","b7ee686b26874244bcf29d7e472bb452","85ee5b3076024f0e9dd582e7e5aa064e","2d5ef34935754853b48c659579f9f8f7","eec8cc9e2c884512a3b99360105207cf","8c7549eba91f4852ad9806492dff7300","b935142ac40e4f1c901ce16e7dae7b52","a4bb51d5f9f84925a8edf2fc86e76135","de45a88569a04711a88463ba953e1dac","caa70790957f49f697b6fcfc5b239efe","3017032257cf4bbfad974de0386656ef","ed8f992a745743cc8849d54fb71dce8f","d5fefc6ba60d4ec084f487fcdddb7fe5","525a3d162baa400987e61175272da739","a226045f68644d06851eebe8f4a53e73","955e6ccf30e74410bd769de447302273","a1864bb7204f4d4091fa76f4211fffad","cc1ca3c4a70d4bc3a9040c0911f8d0bf"]},"id":"cJU6yv3wRW8j","outputId":"9afc0b36-3c13-4ee1-e278-1235d15d5a27"},"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n"]},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/garrett361/large_abstract_recurrent_one_hot_next_language_model/runs/5yxg48v3\" target=\"_blank\">lunar-water-30</a></strong> to <a href=\"https://wandb.ai/garrett361/large_abstract_recurrent_one_hot_next_language_model\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py:342: UserWarning:\n","\n","There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","\n","\n","  | Name               | Type       | Params\n","--------------------------------------------------\n","0 | train_metrics_dict | ModuleDict | 0     \n","1 | val_metrics_dict   | ModuleDict | 0     \n","2 | test_metrics_dict  | ModuleDict | 0     \n","3 | rnn                | GRU        | 2.5 M \n","4 | fc_layers          | ModuleList | 35.4 K\n","--------------------------------------------------\n","2.5 M     Trainable params\n","0         Non-trainable params\n","2.5 M     Total params\n","10.027    Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19f0584223e748559c91fc00e2f68d17","version_minor":0,"version_major":2},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved best val_acc at global step: 0\n","Epoch: 0\n","Validation accuracy: 0.0024781227111816406\n","Validation Loss: 4.23317813873291\n","Saved best val_loss at global step: 0\n","Epoch: 0\n","Validation accuracy: 0.0024781227111816406\n","Validation Loss: 4.23317813873291\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4bb51d5f9f84925a8edf2fc86e76135","version_minor":0,"version_major":2},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch   608: reducing learning rate of group 0 to 5.0000e-03.\n","Epoch   831: reducing learning rate of group 0 to 2.5000e-03.\n","Epoch   875: reducing learning rate of group 0 to 1.2500e-03.\n","Epoch   944: reducing learning rate of group 0 to 6.2500e-04.\n","Epoch   977: reducing learning rate of group 0 to 3.1250e-04.\n","Epoch  1010: reducing learning rate of group 0 to 1.5625e-04.\n","Epoch  1072: reducing learning rate of group 0 to 7.8125e-05.\n"]}],"source":["trainer = Trainer(logger=WandbLogger(),\n","                  gpus=-1 if IS_CUDA_AVAILABLE else 0,\n","                  log_every_n_steps=1,\n","                  callbacks=[notebook_wandb_callback()]\n","                  )\n","with wandb.init(project=PROJECT) as run:\n","    run.name = f\"lr_{model.hparams['lr']}_scheduler_{model_args_dict.get('lr_scheduler', None)}\"[:128]\n","    trainer.fit(model, datamodule=datamodule)\n","    plt.close(\"all\")\n"]},{"cell_type":"markdown","metadata":{"id":"VGAXPFMzg2QT"},"source":["# Loading Best Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmzGOAg7g40x"},"outputs":[],"source":["wandb_api = wandb.Api()\n","notebook_runs = wandb_api.runs(ENTITY + \"/\" + PROJECT) \n","\n","run_cats = ('best_val_acc','config', 'name', 'wandb_path')\n","notebook_runs_dict = {key: [] for key in run_cats}\n","\n","for run in notebook_runs:\n","    run_json = run.summary._json_dict\n","    if 'best_val_acc' in run_json:\n","        notebook_runs_dict['best_val_acc'].append(run_json['best_val_acc'])\n","        notebook_runs_dict['config'].append({key: val for key, val in run.config.items()})\n","        notebook_runs_dict['name'].append(run.name)\n","        notebook_runs_dict['wandb_path'].append('/'.join(run.path))\n","    \n","notebook_runs_df = pd.DataFrame(notebook_runs_dict).sort_values(by='best_val_acc', ascending=False).reset_index(drop=True)\n","notebook_runs_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vKX8-YSnnJV"},"outputs":[],"source":["best_model_df = notebook_runs_df.iloc[notebook_runs_df['best_val_acc'].argmax()]\n","print(best_model_df)"]},{"cell_type":"markdown","metadata":{"id":"JWHtDEyrnnJY"},"source":["Save the state dicts locally and rebuild the corresponding models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8uEeZcLnnJZ"},"outputs":[],"source":["# wandb stores None values in the config dict as a string literal. Need to\n","# fix these entries, annoyingly.\n","for key, val in best_model_df.config.items():\n","    if val == 'None':\n","        best_model_df.config[key] = None\n","# Write to disk\n","best_model_file_name = f\"model_best_val_acc.pt\"\n","wandb.restore(best_model_file_name,\n","              run_path=best_model_df.wandb_path,\n","              replace=True)\n","best_model_file_name_suffix = '_'.join(best_model_file_name.split('_')[-2:])\n","# Also copy to the final_models folder\n","!cp '{best_model_file_name}' \"{FOLDERNAME + '/final_models/' + PROJECT + '_' + best_model_file_name_suffix}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stPhRPeBnnJd"},"outputs":[],"source":["best_model = notebook_model(**{**best_model_df.config, **{'tokens': tokens_df}})\n","best_model.load_state_dict(torch.load(best_model_file_name))"]},{"cell_type":"markdown","metadata":{"id":"GdWSVBrOMJ2X"},"source":["# Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_46Qh7dk8IJ"},"outputs":[],"source":["heatmap = avm.embedding_cosine_heatmap(model=best_model,\n","                                       words=heatmap_words,\n","                                       word_to_idx=title_word_to_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5piTr2-CmjHa"},"outputs":[],"source":["pca = avm.pca_3d_embedding_plotter_topk(model=best_model,\n","                                     words=pca_words,\n","                                     word_to_idx=title_word_to_idx,\n","                                     idx_to_word=title_idx_to_word,\n","                                     title='PCA',\n","                                     k=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T6h2Pv-mwyG"},"outputs":[],"source":["tsne = avm.tsne_3d_embedding_plotter_topk(model=best_model,\n","                                     words=tsne_words,\n","                                     word_to_idx=title_word_to_idx,\n","                                     idx_to_word=title_idx_to_word,\n","                                     title='t-SNE',\n","                                     k=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJziDqL2SV9u"},"outputs":[],"source":["pca.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AIsi4-dJBUk"},"outputs":[],"source":["tsne.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHyd8lx3pZJR"},"outputs":[],"source":["avm.embedding_utils.topk_analogies_df(best_model,\n","                                      'newton mechanics heisenberg'.split(),\n","                                      title_word_to_idx,\n","                                      title_idx_to_word)"]}]}
