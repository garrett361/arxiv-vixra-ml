{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopie van Kopie van Kopie van Kopie van large_abstract_recurrent_one_hot_next_language_model.ipynb","provenance":[{"file_id":"1XCIuPBVtvIllx6TLL78gOhA6Kq-2LrOL","timestamp":1636404967872}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP3UFZQR8DLedZQeu9EgJYA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"33c95407539b4519b82d8ec50f50acc8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bac98506ed7a4200ab0b3d94ca82b7ac","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d2fc80a3fdbe4897bb860dad3ad2b8b5","IPY_MODEL_071a6ae719a64e8c92b1b5d7fff75b81","IPY_MODEL_46cb7f8e5cb240b6ab8329dea23fb1fa"]}},"bac98506ed7a4200ab0b3d94ca82b7ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"d2fc80a3fdbe4897bb860dad3ad2b8b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_767444dce80b4040b36e731e919edfca","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Validation sanity check: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a3a788cbfeb4bd99d98f4ef65582a61"}},"071a6ae719a64e8c92b1b5d7fff75b81":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_443940bf1cc54384abb4f12a15d3128a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e466c6dbe6864fcaa8c195bee389ca67"}},"46cb7f8e5cb240b6ab8329dea23fb1fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0448e339a8da4b15b223140a9f60c25f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:04&lt;00:00,  2.06s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c378276174934c56806fc92c533c3532"}},"767444dce80b4040b36e731e919edfca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0a3a788cbfeb4bd99d98f4ef65582a61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"443940bf1cc54384abb4f12a15d3128a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e466c6dbe6864fcaa8c195bee389ca67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0448e339a8da4b15b223140a9f60c25f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c378276174934c56806fc92c533c3532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc51ecac1ef74de68daba3405916f1b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5f75899f28c94f69b31a0326939e56b7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e540cb74c67f4cb4ac5d87e8155e4806","IPY_MODEL_78791354cc69433b8e8bf4b50827146e","IPY_MODEL_ebd1b14e5c504e0b877abcc4bbe87d96"]}},"5f75899f28c94f69b31a0326939e56b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"e540cb74c67f4cb4ac5d87e8155e4806":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d66df5a15921471e95c5b46d410cd3cd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch 0:   0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5faf0908af3f42fd8c32fa5fe6f0a8cb"}},"78791354cc69433b8e8bf4b50827146e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7059c38f35284e94891fc047264f598f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1429,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90d8c5101f644d4aab43bc806e103773"}},"ebd1b14e5c504e0b877abcc4bbe87d96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b813c26fd1134e998a8919f64f7afc90","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1429 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f1e7c76886d4c189dd90a5220fabcb8"}},"d66df5a15921471e95c5b46d410cd3cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5faf0908af3f42fd8c32fa5fe6f0a8cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7059c38f35284e94891fc047264f598f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"90d8c5101f644d4aab43bc806e103773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b813c26fd1134e998a8919f64f7afc90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9f1e7c76886d4c189dd90a5220fabcb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"YtasH2tBWrTK"},"source":["# Simple Recurrent Language Model\n","\n","Predicting the next token."]},{"cell_type":"markdown","metadata":{"id":"z225dbwoaygs"},"source":["# Imports and Setup\n","\n","Common imports and standardized code for importing the relevant data, models, etc., in order to minimize copy-paste/typo errors."]},{"cell_type":"markdown","metadata":{"id":"cBD3U5YCt7-s"},"source":["\n","Set the relevant text field (`'abstract'` or `'title'`) and whether we are working with `'one-hot'` or `'tokenized'` text.  "]},{"cell_type":"code","metadata":{"id":"7N6wzc1S1f91"},"source":["TEXT_FIELD = 'abstract'\n","TEXT_ENCODING = 'one-hot'\n","assert TEXT_FIELD in ('abstract', 'title'), 'TEXT_FIELD must be one of \"title\" or \"abstract\".'\n","assert TEXT_ENCODING in ('one-hot', 'tokenized'), 'TEXT_ENCODING must be one of \"one-hot\" or \"tokenized\".'\n","# The above choices determine the relevant sequence length of the data.\n","SEQ_LEN = 512 if TEXT_ENCODING == 'tokenized' else 1024"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfUISDFhbmZB"},"source":["Imports and colab setup"]},{"cell_type":"code","metadata":{"id":"beaGM0OZDGBe"},"source":["%%capture import_capture --no-stder\n","# Jupyter magic methods\n","# For auto-reloading when external modules are changed\n","%load_ext autoreload\n","%autoreload 2\n","# For showing plots inline\n","%matplotlib inline\n","\n","# pip installs needed in Colab for arxiv_vixra_models\n","!pip install wandb\n","!pip install pytorch-lightning\n","!pip install unidecode\n","# Update sklearn\n","!pip uninstall scikit-learn -y\n","!pip install -U scikit-learn\n","\n","from copy import deepcopy\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","pd.set_option(u'float_format', '{:f}'.format)\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","import seaborn as sns\n","import torch\n","import wandb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmuHdurUyvjK"},"source":["`wandb` log in:"]},{"cell_type":"code","metadata":{"id":"3DfWm5EfyyV-","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1642476906085,"user_tz":300,"elapsed":5340,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"3696682f-fa2c-49a7-b339-a825e166f4f8"},"source":["wandb.login()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"kamviAnIb068"},"source":["Google drive access"]},{"cell_type":"code","metadata":{"id":"Q2rHoHQkVKqH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642476929169,"user_tz":300,"elapsed":23108,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"c9d33d1b-c0e8-48a5-ae73-5bfde1f6097a"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","# Enter the relevant foldername\n","FOLDERNAME = '/content/drive/My Drive/ML/arxiv_vixra'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","# For importing modules stored in FOLDERNAME or a subdirectory thereof:\n","import sys\n","sys.path.append(FOLDERNAME)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"K9ga5ia9cFRw"},"source":["Import my models, loaders, and utility functions:"]},{"cell_type":"code","metadata":{"id":"j8U3Ki7mbcw7"},"source":["import arxiv_vixra_models as avm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CXP2Q2FoaryB"},"source":["Set the model, datamodule, and text utils to be instantianted in the notebook"]},{"cell_type":"code","metadata":{"id":"UZfn8eELa0sg"},"source":["notebook_model = avm.LitOneHotCharRNNNextLM\n","notebook_datamodule = avm.OneHotCharDataModuleNextLM\n","notebook_encoder = avm.str_to_one_hot \n","notebook_decoder = avm.one_hot_to_str \n","notebook_wandb_callback = avm.WandbTextGenerationCallback"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAjCZ39tcQ1Y"},"source":["Copy data to cwd for speed."]},{"cell_type":"code","metadata":{"id":"V4b1KZSYV8rP"},"source":["train_data_file_name = 'large_filtered_normalized_data_train.feather'\n","val_data_file_name = 'balanced_filtered_normalized_data_validation.feather'\n","SUBDIR = '/data/data_splits/'\n","train_data_path = FOLDERNAME + SUBDIR + train_data_file_name\n","val_data_path = FOLDERNAME + SUBDIR + val_data_file_name\n","if TEXT_ENCODING == 'one-hot':\n","    tokens_file_name = 'normalized_char_set.feather'\n","else:\n","    tokens_file_name = 'balanced_title_normalized_vocab.feather'\n","tokens_path = FOLDERNAME + SUBDIR + tokens_file_name\n","!cp '{train_data_path}' .\n","!cp '{val_data_path}' .\n","!cp '{tokens_path}' .\n","train_data_df = pd.read_feather(train_data_file_name)\n","val_data_df = pd.read_feather(val_data_file_name)\n","tokens_df = pd.read_feather(tokens_file_name)\n","if TEXT_ENCODING == 'one-hot':\n","    text_to_idx = dict(zip(tokens_df.char.values, np.arange(len(tokens_df))))\n","else:\n","    # 0 and 1 are reserved for padding and <UNK> for embeddings and not included\n","    # in tokens_df\n","    text_to_idx = dict(zip(tokens_df.word.values, np.arange(2, len(tokens_df) + 2)))\n","    text_to_idx['<PAD>'] = 0\n","    text_to_idx['<UNK>'] = 1\n","idx_to_text = {val: key for key, val in text_to_idx.items()}\n","if TEXT_FIELD == 'title':\n","    train_text_file_name = 'concatenated_large_normalized_train_title.txt'\n","    val_text_file_name = 'concatenated_balanced_normalized_validation_title.txt'\n","else:\n","    train_text_file_name = 'concatenated_large_normalized_train_abstract.txt'\n","    val_text_file_name = 'concatenated_balanced_normalized_validation_abstract.txt'\n","with open(FOLDERNAME + SUBDIR + train_text_file_name, 'r') as f:\n","    train_text = f.read().strip()\n","with open(FOLDERNAME + SUBDIR + val_text_file_name, 'r') as f:\n","    val_text = f.read().strip()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRfQtriZWa1P"},"source":["Computing specs. Save the number of processors to pass as `num_workers` into the Datamodule and cuda availability for other flags."]},{"cell_type":"code","metadata":{"id":"yuUpH52TUAG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642476965174,"user_tz":300,"elapsed":426,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"ce03fd58-e30d-4cb4-aa96-fa581bda5966"},"source":["# GPU. Save availability to IS_CUDA_AVAILABLE.\n","gpu_info= !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","  IS_CUDA_AVAILABLE = False\n","else:\n","  print(f\"GPU\\n{50 * '-'}\\n\", gpu_info, '\\n')\n","  IS_CUDA_AVAILABLE = True\n","\n","# Memory.\n","from psutil import virtual_memory, cpu_count\n","ram_gb = virtual_memory().total / 1e9\n","print(f\"Memory\\n{50 * '-'}\\n\", 'Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb), '\\n')\n","\n","# CPU.\n","print(f\"CPU\\n{50 * '-'}\\n\", f'CPU Processors: {cpu_count()}')\n","# Determine the number of workers to use in the datamodule\n","NUM_PROCESSORS = cpu_count()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU\n","--------------------------------------------------\n"," Tue Jan 18 03:36:04 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+ \n","\n","Memory\n","--------------------------------------------------\n"," Your runtime has 54.8 gigabytes of available RAM\n"," \n","\n","CPU\n","--------------------------------------------------\n"," CPU Processors: 8\n"]}]},{"cell_type":"markdown","metadata":{"id":"yP-xjLY4cEEL"},"source":["Use notebook name as `wandb` `project` string. Remove the file extension and any \"Copy of\" or \"Kopie van\" text which arises from copying notebooks and running in parallel. The `entity` needed for various `wandb` calls is just the `wandb` user name."]},{"cell_type":"code","metadata":{"id":"yy8UZlYodrhO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642476965175,"user_tz":300,"elapsed":9,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"f9e38dcd-9687-49da-da83-c65ec1ef7b63"},"source":["from requests import get\n","PROJECT = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n","PROJECT = PROJECT.replace('.ipynb', '').replace('Kopie%20van%20', '').replace('Copy%20of%20', '')\n","print(PROJECT)\n","ENTITY = 'garrett361'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["large_abstract_recurrent_one_hot_next_language_model\n"]}]},{"cell_type":"markdown","metadata":{"id":"2HVB4e-xchSi"},"source":["# Model Testing\n","\n","Setting hyperparameters and performing a small test run."]},{"cell_type":"markdown","metadata":{"id":"E0e7seomcjud"},"source":["Dictionary args for model and datamodule."]},{"cell_type":"code","metadata":{"id":"GLg5mLHchpqp"},"source":["model_args_dict = {'seq_len': SEQ_LEN,\n","                  'tokens': tokens_df,\n","                  'num_layers': 2,\n","                  'hidden_size': 512,\n","                  'rnn_type': 'GRU',\n","                  'fc_dims': None,\n","                  'zero_fc_bias_init': True,\n","                  'truncated_bptt_steps': 128\n","                  }\n","\n","data_args_dict = {'seq_len': SEQ_LEN,\n","                 'train_text': train_text,\n","                 'val_text': val_text,\n","                 'tokens': tokens_df, \n","                 'num_workers': NUM_PROCESSORS,\n","                 'batch_size': 128,\n","                 'pin_memory': IS_CUDA_AVAILABLE,\n","                 'persistent_workers': True,\n","                 }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okEcWXWWxYb0"},"source":["Small test run."]},{"cell_type":"code","metadata":{"id":"qhOY0cnMvnX-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642476989055,"user_tz":300,"elapsed":23885,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"185532c5-3e15-45ce-fc79-ef5f71ffad8b"},"source":["small_data_module = notebook_datamodule(**data_args_dict)\n","small_data_module.setup()\n","small_loader = small_data_module.train_dataloader()\n","small_inputs, small_targets = next(iter(small_loader))\n","# Print the first few input texts\n","for input, target in  zip(small_inputs[:3], small_targets[:3]):\n","    sample_text = notebook_decoder(input, idx_to_text)\n","    sample_target = ''.join(idx_to_text[ch.item()] for ch in target)\n","    print(f\"input  text: {sample_text}\",\n","          f\"target text: {sample_target}\",\n","          f'input, target lens: {len(sample_text), len(sample_target)}',\n","          sep='\\n')\n","small_model = notebook_model(**model_args_dict)\n","print('Model layers:', small_model)\n","small_preds, small_losses, _ = small_model.scores_loss_hiddens(small_inputs, small_targets)\n","print('\\npreds shape:', small_preds.shape)\n","print('\\nactual loss:', small_losses.item())\n","print('\\nexpected approx loss', np.log(len(tokens_df)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input  text:  a general - purpose method for finding high - quality solutions to hard optimization problems , inspired by self - organizing processes often found in nature . the method , called extremal optimization , successively eliminates extremely undesirable components of sub - optimal solutions . drawing upon models used to simulate far - from - equilibrium dynamics , it complements approximation methods inspired by equilibrium statistical physics , such as simulated annealing . with only one adjustable parameter , its performance proves competitive with , and often superior to , more elaborate stochastic optimization procedures . we demonstrate it here on two classic hard optimization problems : graph partitioning and the traveling salesman problem . we study numerically phonon modes of the classical one - dimensional frenkel - kontorova chain , in the regime of pinned phase characterized by the phonon gap and devil ' s staircase , as well as by a large number of states ( configurational excitations ) , which energ\n","target text: a general - purpose method for finding high - quality solutions to hard optimization problems , inspired by self - organizing processes often found in nature . the method , called extremal optimization , successively eliminates extremely undesirable components of sub - optimal solutions . drawing upon models used to simulate far - from - equilibrium dynamics , it complements approximation methods inspired by equilibrium statistical physics , such as simulated annealing . with only one adjustable parameter , its performance proves competitive with , and often superior to , more elaborate stochastic optimization procedures . we demonstrate it here on two classic hard optimization problems : graph partitioning and the traveling salesman problem . we study numerically phonon modes of the classical one - dimensional frenkel - kontorova chain , in the regime of pinned phase characterized by the phonon gap and devil ' s staircase , as well as by a large number of states ( configurational excitations ) , which energy\n","input, target lens: (1024, 1024)\n","input  text: nted . this embedding enables the mechanisation and automation of reasoning tasks in input / output logic with off - the - shelf higher - order theorem provers and proof assistants . the key idea for the solution presented here results from the analysis of an inaccurate previous embedding attempt , which we will discuss as well . a recent goal in the reinforcement learning ( rl ) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon . for several rl problems in operation research and optimal control , the optimal policy of the underlying markov decision process ( mdp ) is characterized by a known structure . the current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret . in this work , we develop new rl algorithms that exploit the structure of the optimal policy to minimize regret . numerical experiments on mdps with structured optimal policies show that our \n","target text: ted . this embedding enables the mechanisation and automation of reasoning tasks in input / output logic with off - the - shelf higher - order theorem provers and proof assistants . the key idea for the solution presented here results from the analysis of an inaccurate previous embedding attempt , which we will discuss as well . a recent goal in the reinforcement learning ( rl ) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon . for several rl problems in operation research and optimal control , the optimal policy of the underlying markov decision process ( mdp ) is characterized by a known structure . the current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret . in this work , we develop new rl algorithms that exploit the structure of the optimal policy to minimize regret . numerical experiments on mdps with structured optimal policies show that our a\n","input, target lens: (1024, 1024)\n","input  text: ) is satisfied , while the isentropic mode is stabilized ( destabilized ) when the opposite inequalities take place ; the isochoric mode is unaltered . numerical estimates show that these effects can be important in hot phases ( $ t \\ sim 10 ^ { 6 } $ k ) of the interstellar plasma , and in tokamak plasma near the walls . we study atomic measures on $ [ 0 , 1 ] $ which are invariant both under multiplication by $ 2 \\ mod 1 $ and by $ 3 \\ mod 1 $ , since such measures play an important role in deciding furstenberg ' s $ \\ times 2 , \\ times 3 $ conjecture . our specific focus was finding atomic measures whose supports are far from being uniformly distributed , and we used computer software to discover a number of such measures ( which we call \\ emph { outlier measures } ) . the structure of these measures indicates the possibility that a sequence of atomic measures may converge to a non - lebesgue measure ; likely one which is a combination of the lebesgue measure and one or more atomic measures . this paper pr\n","target text:  is satisfied , while the isentropic mode is stabilized ( destabilized ) when the opposite inequalities take place ; the isochoric mode is unaltered . numerical estimates show that these effects can be important in hot phases ( $ t \\ sim 10 ^ { 6 } $ k ) of the interstellar plasma , and in tokamak plasma near the walls . we study atomic measures on $ [ 0 , 1 ] $ which are invariant both under multiplication by $ 2 \\ mod 1 $ and by $ 3 \\ mod 1 $ , since such measures play an important role in deciding furstenberg ' s $ \\ times 2 , \\ times 3 $ conjecture . our specific focus was finding atomic measures whose supports are far from being uniformly distributed , and we used computer software to discover a number of such measures ( which we call \\ emph { outlier measures } ) . the structure of these measures indicates the possibility that a sequence of atomic measures may converge to a non - lebesgue measure ; likely one which is a combination of the lebesgue measure and one or more atomic measures . this paper pre\n","input, target lens: (1024, 1024)\n","Model layers: LitOneHotCharRNNNextLM(\n","  (train_metrics_dict): ModuleDict(\n","    (train_acc): Accuracy()\n","    (train_precision): Precision()\n","    (train_recall): Recall()\n","    (train_specificity): Specificity()\n","  )\n","  (val_metrics_dict): ModuleDict(\n","    (val_acc): Accuracy()\n","    (val_precision): Precision()\n","    (val_recall): Recall()\n","    (val_specificity): Specificity()\n","  )\n","  (test_metrics_dict): ModuleDict(\n","    (test_acc): Accuracy()\n","    (test_precision): Precision()\n","    (test_recall): Recall()\n","    (test_specificity): Specificity()\n","  )\n","  (rnn): GRU(69, 512, num_layers=2, batch_first=True)\n","  (fc_layers): ModuleList(\n","    (0): Linear(in_features=512, out_features=69, bias=True)\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/deprecate/deprecation.py:115: FutureWarning:\n","\n","The `F1` was deprecated since v0.7 in favor of `torchmetrics.classification.f_beta.F1Score`. It will be removed in v0.8.\n","\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","preds shape: torch.Size([128, 69, 1024])\n","\n","actual loss: 4.240167617797852\n","\n","expected approx loss 4.23410650459726\n"]}]},{"cell_type":"code","metadata":{"id":"lLdlEdmO83mU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642476989059,"user_tz":300,"elapsed":39,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"b19a787d-ff0e-41f1-f1db-a36e6f68f9f5"},"source":["# pl implements gradient clipping through the Trainer.\n","small_trainer = Trainer(gpus=-1 if IS_CUDA_AVAILABLE else 0,\n","                        max_epochs=1,\n","                        gradient_clip_val=1\n","                        )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n"]}]},{"cell_type":"markdown","metadata":{"id":"97-nbRSP87fY"},"source":["A `LR finder stopped early due to diverging loss.` here may be due to having too large a batch size, i.e., not enough samples from the datamodule; [see this github discussion](https://github.com/PyTorchLightning/pytorch-lightning/issues/5044)"]},{"cell_type":"code","metadata":{"id":"wE0laRH4samb"},"source":["# small_trainer_lr_finder = small_trainer.tuner.lr_find(small_model, datamodule=small_data_module, min_lr=1e-6, max_lr=1e-1)\n","# small_trainer_lr_finder_plot = small_trainer_lr_finder.plot(suggest=True)\n","# small_trainer_suggested_lr = small_trainer_lr_finder.suggestion()\n","# print(f'Suggested lr: {small_trainer_suggested_lr}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"b7DPjsjTnlso"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjBi9bSrYa4b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642476989060,"user_tz":300,"elapsed":23,"user":{"displayName":"Garrett Goon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiiYFf8nm-IpkI8yUgW_CqJJ76xMZhUUU94FCJabg=s64","userId":"15364959356591490410"}},"outputId":"a6b4510a-4273-45ab-e35c-427146740f4a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning:\n","\n","Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","\n"]}],"source":["cyclic_lr_scheduler_args = {'base_lr': 1e-4,\n","                            'max_lr': 1e-2,\n","                            'step_size_up': 512,\n","                            'mode': 'triangular2',\n","                            'cycle_momentum': False}\n","plateau_lr_scheduler_args = {'verbose': True,\n","                             'patience': 512,\n","                             'factor': .5,\n","                             'mode': 'min'}\n","\n","model_args_dict['save_models_to_wandb'] =True\n","model_args_dict['lr'] = 1e-2\n","model_args_dict['lr_scheduler'] = 'cyclic'\n","model_args_dict['lr_scheduler_args'] = cyclic_lr_scheduler_args\n","model_args_dict['lr_scheduler_monitor'] = 'train_batch_loss'\n","model = notebook_model(**model_args_dict)\n","\n","data_args_dict['batch_size'] = 1024\n","datamodule = notebook_datamodule(**data_args_dict)"]},{"cell_type":"markdown","metadata":{"id":"R2nS87PG92Aw"},"source":["Training:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":594,"referenced_widgets":["33c95407539b4519b82d8ec50f50acc8","bac98506ed7a4200ab0b3d94ca82b7ac","d2fc80a3fdbe4897bb860dad3ad2b8b5","071a6ae719a64e8c92b1b5d7fff75b81","46cb7f8e5cb240b6ab8329dea23fb1fa","767444dce80b4040b36e731e919edfca","0a3a788cbfeb4bd99d98f4ef65582a61","443940bf1cc54384abb4f12a15d3128a","e466c6dbe6864fcaa8c195bee389ca67","0448e339a8da4b15b223140a9f60c25f","c378276174934c56806fc92c533c3532","cc51ecac1ef74de68daba3405916f1b6","5f75899f28c94f69b31a0326939e56b7","e540cb74c67f4cb4ac5d87e8155e4806","78791354cc69433b8e8bf4b50827146e","ebd1b14e5c504e0b877abcc4bbe87d96","d66df5a15921471e95c5b46d410cd3cd","5faf0908af3f42fd8c32fa5fe6f0a8cb","7059c38f35284e94891fc047264f598f","90d8c5101f644d4aab43bc806e103773","b813c26fd1134e998a8919f64f7afc90","9f1e7c76886d4c189dd90a5220fabcb8"]},"id":"cJU6yv3wRW8j","outputId":"0cc7637f-8910-41a6-9188-381da49f5c8c"},"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgarrett361\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/garrett361/large_abstract_recurrent_one_hot_next_language_model/runs/3bja447a\" target=\"_blank\">smooth-firebrand-44</a></strong> to <a href=\"https://wandb.ai/garrett361/large_abstract_recurrent_one_hot_next_language_model\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py:342: UserWarning:\n","\n","There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","\n","\n","  | Name               | Type       | Params\n","--------------------------------------------------\n","0 | train_metrics_dict | ModuleDict | 0     \n","1 | val_metrics_dict   | ModuleDict | 0     \n","2 | test_metrics_dict  | ModuleDict | 0     \n","3 | rnn                | GRU        | 2.5 M \n","4 | fc_layers          | ModuleList | 35.4 K\n","--------------------------------------------------\n","2.5 M     Trainable params\n","0         Non-trainable params\n","2.5 M     Total params\n","10.027    Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33c95407539b4519b82d8ec50f50acc8","version_minor":0,"version_major":2},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved best val_acc at global step: 0\n","Epoch: 0\n","Validation accuracy: 0.005822181701660156\n","Validation Loss: 4.230160713195801\n","Saved best val_loss at global step: 0\n","Epoch: 0\n","Validation accuracy: 0.005822181701660156\n","Validation Loss: 4.230160713195801\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc51ecac1ef74de68daba3405916f1b6","version_minor":0,"version_major":2},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{}}],"source":["# We accumulate gradients in batches to help smooth out the loss-curve.\n","trainer = Trainer(logger=WandbLogger(),\n","                  gpus=-1 if IS_CUDA_AVAILABLE else 0,\n","                  log_every_n_steps=1,\n","                  callbacks=[notebook_wandb_callback()],\n","                  gradient_clip_val=1,\n","                  )\n","with wandb.init(project=PROJECT) as run:\n","    run.name = f\"lr_{model.hparams['lr']}_scheduler_{model_args_dict.get('lr_scheduler', None)}\"[:128]\n","    trainer.fit(model, datamodule=datamodule)\n","    plt.close(\"all\")\n"]},{"cell_type":"markdown","metadata":{"id":"VGAXPFMzg2QT"},"source":["# Loading Best Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmzGOAg7g40x"},"outputs":[],"source":["wandb_api = wandb.Api()\n","notebook_runs = wandb_api.runs(ENTITY + \"/\" + PROJECT) \n","\n","run_cats = ('best_val_acc','config', 'name', 'wandb_path')\n","notebook_runs_dict = {key: [] for key in run_cats}\n","\n","for run in notebook_runs:\n","    run_json = run.summary._json_dict\n","    if 'best_val_acc' in run_json:\n","        notebook_runs_dict['best_val_acc'].append(run_json['best_val_acc'])\n","        notebook_runs_dict['config'].append({key: val for key, val in run.config.items()})\n","        notebook_runs_dict['name'].append(run.name)\n","        notebook_runs_dict['wandb_path'].append('/'.join(run.path))\n","    \n","notebook_runs_df = pd.DataFrame(notebook_runs_dict).sort_values(by='best_val_acc', ascending=False).reset_index(drop=True)\n","notebook_runs_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vKX8-YSnnJV"},"outputs":[],"source":["best_model_df = notebook_runs_df.iloc[notebook_runs_df['best_val_acc'].argmax()]\n","print(best_model_df)"]},{"cell_type":"markdown","metadata":{"id":"JWHtDEyrnnJY"},"source":["Save the state dicts locally and rebuild the corresponding models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8uEeZcLnnJZ"},"outputs":[],"source":["# wandb stores None values in the config dict as a string literal. Need to\n","# fix these entries, annoyingly.\n","for key, val in best_model_df.config.items():\n","    if val == 'None':\n","        best_model_df.config[key] = None\n","# Write to disk\n","best_model_file_name = f\"model_best_val_acc.pt\"\n","wandb.restore(best_model_file_name,\n","              run_path=best_model_df.wandb_path,\n","              replace=True)\n","best_model_file_name_suffix = '_'.join(best_model_file_name.split('_')[-2:])\n","# Also copy to the final_models folder\n","!cp '{best_model_file_name}' \"{FOLDERNAME + '/final_models/' + PROJECT + '_' + best_model_file_name_suffix}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stPhRPeBnnJd"},"outputs":[],"source":["best_model = notebook_model(**{**best_model_df.config, **{'tokens': tokens_df}})\n","best_model.load_state_dict(torch.load(best_model_file_name))"]},{"cell_type":"markdown","metadata":{"id":"GdWSVBrOMJ2X"},"source":["# Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_46Qh7dk8IJ"},"outputs":[],"source":["heatmap = avm.embedding_cosine_heatmap(model=best_model,\n","                                       words=heatmap_words,\n","                                       word_to_idx=title_word_to_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5piTr2-CmjHa"},"outputs":[],"source":["pca = avm.pca_3d_embedding_plotter_topk(model=best_model,\n","                                     words=pca_words,\n","                                     word_to_idx=title_word_to_idx,\n","                                     idx_to_word=title_idx_to_word,\n","                                     title='PCA',\n","                                     k=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T6h2Pv-mwyG"},"outputs":[],"source":["tsne = avm.tsne_3d_embedding_plotter_topk(model=best_model,\n","                                     words=tsne_words,\n","                                     word_to_idx=title_word_to_idx,\n","                                     idx_to_word=title_idx_to_word,\n","                                     title='t-SNE',\n","                                     k=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJziDqL2SV9u"},"outputs":[],"source":["pca.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AIsi4-dJBUk"},"outputs":[],"source":["tsne.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHyd8lx3pZJR"},"outputs":[],"source":["avm.embedding_utils.topk_analogies_df(best_model,\n","                                      'newton mechanics heisenberg'.split(),\n","                                      title_word_to_idx,\n","                                      title_idx_to_word)"]}]}
